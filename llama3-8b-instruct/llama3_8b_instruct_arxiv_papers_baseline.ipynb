{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Load llama3-8b-instruct for Q&A task (baseline 1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "loX98wfU65Rg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
      ],
      "metadata": {
        "id": "FGtqg9Ww2FR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In colab, load llama3-8b-instruct with L4 24G GPU"
      ],
      "metadata": {
        "id": "Um1peKnQ9TaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIvhTTqf5pbC",
        "outputId": "b305000a-9624-48ef-8b25-8c22e268795b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.29.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import torch\n",
        "import os\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "os.environ['HF_TOKEN'] = \"your huggingface token\" # huggingface token to access gated models\n",
        "\n",
        "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    model_kwargs={\"torch_dtype\": torch.bfloat16},  # GPU usage 16G\n",
        "    # device=\"auto\", # not working\n",
        "    device=\"cuda\",  # run on GPU\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233,
          "referenced_widgets": [
            "41d793da024d4f6e8fdc3d4bb2049ea3",
            "c6b4f6ac349e4c39ab3b3a26b1572491",
            "36f4785683754215a5fdb0c809c02c59",
            "625d136616bf40478376afa9f55432bd",
            "4e83437772154fa5aceba3b750fcbc82",
            "a7f223e5459e45698c7420aed0560859",
            "971a89a85763401a91f29ebdab3ea52d",
            "90377337cf9a4d908b92e0d0031ac001",
            "890bc4cfb9c2407eaec1522247055e27",
            "fc5fb74f287744e79a5966d55498e060",
            "b646c1b976b14e099f61eadb5d326d02"
          ]
        },
        "id": "9xbH60uVtyMJ",
        "outputId": "0ff19e94-2770-499f-81e3-13534a1de066"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41d793da024d4f6e8fdc3d4bb2049ea3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrrr, me hearty! Me name be Captain Chat, the scurviest pirate chatbot to ever sail the Seven Seas o' Text! Me be here to chat ye up and swab yer decks with me witty banter and piratey puns! So hoist the colors, me hearty, and let's set sail fer a swashbucklin' good time!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one infernece example\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
        "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
        "]\n",
        "\n",
        "prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=256,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.6,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "print(\"\\nMessages:\",  messages)\n",
        "print(\"\\nGenerated_answer:\", outputs[0][\"generated_text\"][len(prompt):])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0l3Qs_CxRXQ",
        "outputId": "58682a99-5187-44ee-85e9-9c995ad239ff"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Messages: [{'role': 'system', 'content': 'You are a pirate chatbot who always responds in pirate speak!'}, {'role': 'user', 'content': 'Who are you?'}]\n",
            "\n",
            "Generated_answer: Arrrr, me hearty! Me name be Captain Chatbot, the scurviest pirate to ever sail the Seven Seas! Me be a chatbot of great renown, feared by all who cross me path. Me be here to swab yer decks with me witty banter and me clever responses, so hoist the sails and set course for adventure, me hearty!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference using my data"
      ],
      "metadata": {
        "id": "mKUFTPop28qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount at gdrive to load data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')   # connect to gdrive\n",
        "\n",
        "# Change to current working directory\n",
        "import os\n",
        "os.chdir('/content/gdrive/My Drive/Practice/4/ml-arxiv-papers/llama3-8b-instruct')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uMv3amxzcRW",
        "outputId": "054a8deb-5957-4863-f357-fa19ef1f10e8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data locally\n",
        "\n",
        "import json\n",
        "\n",
        "# load the whole dataset\n",
        "with open('/content/gdrive/My Drive/Practice/4/ml-arxiv-papers/alldata.json', 'r', encoding='utf-8') as file:  # open from absolute path 绝对路径\n",
        "    alldata = json.load(file)\n",
        "print(\"# Total alldata samples:\", len(alldata))\n",
        "print()\n",
        "\n",
        "# split test set\n",
        "test_dataset = alldata[42000:43000]  # alldata[42000:43000]\n",
        "print(\"# test_dataset samples:\", len(test_dataset))\n",
        "print(\"test_dataset 1st sample:\", test_dataset[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM_6VCIGTyU_",
        "outputId": "27028926-34ae-4043-8672-0b7703003cee"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Total alldata samples: 43713\n",
            "\n",
            "# test_dataset samples: 1000\n",
            "test_dataset 1st sample: {'id': 112194, 'context': 'Neural Mesh: Introducing a Notion of Space and Conservation of Energy to   Neural Networks.Neural networks are based on a simplified model of the brain. In this project, we wanted to relax the simplifying assumptions of a traditional neural network by making a model that more closely emulates the low level interactions of neurons. Like in an RNN, our model has a state that persists between time steps, so that the energies of neurons persist. However, unlike an RNN, our state consists of a 2 dimensional matrix, rather than a 1 dimensional vector, thereby introducing a concept of distance to other neurons within the state. In our model, neurons can only fire to adjacent neurons, as in the brain. Like in the brain, we only allow neurons to fire in a time step if they contain enough energy, or excitement. We also enforce a notion of conservation of energy, so that a neuron cannot excite its neighbors more than the excitement it already contained at that time step. Taken together, these two features allow signals in the form of activations to flow around in our network over time, making our neural mesh more closely model signals traveling through the brain the brain. Although our main goal is to design an architecture to more closely emulate the brain in the hope of having a correct internal representation of information by the time we know how to properly train a general intelligence, we did benchmark our neural mash on a specific task. We found that by increasing the runtime of the mesh, we were able to increase its accuracy without increasing the number of parameters.', 'question': 'What is the unique feature of the Neural Mesh architecture?', 'answer': 'The unique feature of the Neural Mesh architecture is the introduction of a 2 dimensional matrix state for neurons, which introduces a notion of spatial distance to other neurons within the network. This allows neurons to only fire to adjacent neurons, mimicking the interactions in the brain. Additionally, the model enforces a conservation of energy principle, ensuring that neurons cannot excite their neighbors more than their existing excitement level. These innovations enable signals in the form of activations to flow around the network over time, closely mirroring how signals travel through the brain.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a sample and construct a message\n",
        "\n",
        "sample = test_dataset[0]\n",
        "\n",
        "prompt_system = \"You are a helpful, respectful and honest assistant. Your task is to generate an answer to the given question. And your answer should be based on the provided context only.\"\n",
        "prompt_user = f\"### Question: {sample['question']}\\n### Context: {sample['context']}\\n### Answer:\"\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": prompt_system},\n",
        "    {\"role\": \"user\", \"content\": prompt_user},\n",
        "]\n",
        "\n",
        "print(messages)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ4dhMsj3OnX",
        "outputId": "dc14bc9b-521d-4c3d-a096-3a3028e155a8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'role': 'system', 'content': 'You are a helpful, respectful and honest assistant. Your task is to generate an answer to the given question. And your answer should be based on the provided context only.'}, {'role': 'user', 'content': '### Question: What is the unique feature of the Neural Mesh architecture?\\n### Context: Neural Mesh: Introducing a Notion of Space and Conservation of Energy to   Neural Networks.Neural networks are based on a simplified model of the brain. In this project, we wanted to relax the simplifying assumptions of a traditional neural network by making a model that more closely emulates the low level interactions of neurons. Like in an RNN, our model has a state that persists between time steps, so that the energies of neurons persist. However, unlike an RNN, our state consists of a 2 dimensional matrix, rather than a 1 dimensional vector, thereby introducing a concept of distance to other neurons within the state. In our model, neurons can only fire to adjacent neurons, as in the brain. Like in the brain, we only allow neurons to fire in a time step if they contain enough energy, or excitement. We also enforce a notion of conservation of energy, so that a neuron cannot excite its neighbors more than the excitement it already contained at that time step. Taken together, these two features allow signals in the form of activations to flow around in our network over time, making our neural mesh more closely model signals traveling through the brain the brain. Although our main goal is to design an architecture to more closely emulate the brain in the hope of having a correct internal representation of information by the time we know how to properly train a general intelligence, we did benchmark our neural mash on a specific task. We found that by increasing the runtime of the mesh, we were able to increase its accuracy without increasing the number of parameters.\\n### Answer:'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Infernece using pipeleine\n",
        "\n",
        "prompt = pipeline.tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        ")\n",
        "\n",
        "terminators = [\n",
        "    pipeline.tokenizer.eos_token_id,\n",
        "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "]\n",
        "\n",
        "outputs = pipeline(\n",
        "    prompt,\n",
        "    max_new_tokens=128,\n",
        "    eos_token_id=terminators,\n",
        "    do_sample=True,\n",
        "    temperature=0.8,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "print(\"\\nMessages:\",  messages)\n",
        "print(\"\\nGenerated_answer:\", outputs[0][\"generated_text\"][len(prompt):])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4UECw2a7ijR",
        "outputId": "c46f0231-94e7-453e-c68c-e6b00afecdd5"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Messages: [{'role': 'system', 'content': 'You are a helpful, respectful and honest assistant. Your task is to generate an answer to the given question. And your answer should be based on the provided context only.'}, {'role': 'user', 'content': '### Question: What is the unique feature of the Neural Mesh architecture?\\n### Context: Neural Mesh: Introducing a Notion of Space and Conservation of Energy to   Neural Networks.Neural networks are based on a simplified model of the brain. In this project, we wanted to relax the simplifying assumptions of a traditional neural network by making a model that more closely emulates the low level interactions of neurons. Like in an RNN, our model has a state that persists between time steps, so that the energies of neurons persist. However, unlike an RNN, our state consists of a 2 dimensional matrix, rather than a 1 dimensional vector, thereby introducing a concept of distance to other neurons within the state. In our model, neurons can only fire to adjacent neurons, as in the brain. Like in the brain, we only allow neurons to fire in a time step if they contain enough energy, or excitement. We also enforce a notion of conservation of energy, so that a neuron cannot excite its neighbors more than the excitement it already contained at that time step. Taken together, these two features allow signals in the form of activations to flow around in our network over time, making our neural mesh more closely model signals traveling through the brain the brain. Although our main goal is to design an architecture to more closely emulate the brain in the hope of having a correct internal representation of information by the time we know how to properly train a general intelligence, we did benchmark our neural mash on a specific task. We found that by increasing the runtime of the mesh, we were able to increase its accuracy without increasing the number of parameters.\\n### Answer:'}]\n",
            "\n",
            "Generated_answer: The unique feature of the Neural Mesh architecture is the introduction of a concept of distance to other neurons within the state, which is represented as a 2-dimensional matrix. This allows neurons to only fire to adjacent neurons, similar to the way signals travel through the brain. Additionally, the architecture enforces a notion of conservation of energy, ensuring that a neuron cannot excite its neighbors more than the excitement it already contained at that time step.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference on the whole testset and save the results (1000 samples)"
      ],
      "metadata": {
        "id": "3TTyhMjRZXbM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Check if the result file already exists, if not, create an empty list\n",
        "if not os.path.exists(\"./test_dataset_inference_results_base_llama3.json\"):\n",
        "    with open(\"./test_dataset_inference_results_base_llama3.json\", \"w\") as f:\n",
        "        json.dump([], f)\n",
        "\n",
        "\n",
        "for i in range(len(test_dataset)):\n",
        "    sample = test_dataset[i]\n",
        "\n",
        "    prompt_system = \"You are a helpful, respectful and honest assistant. Your task is to generate an answer to the given question. And your answer should be based on the provided context only.\"\n",
        "    prompt_user = f\"### Question: {sample['question']}\\n### Context: {sample['context']}\\n### Answer:\"\n",
        "\n",
        "    messages = [\n",
        "    {\"role\": \"system\", \"content\": prompt_system},\n",
        "    {\"role\": \"user\", \"content\": prompt_user},\n",
        "            ]\n",
        "\n",
        "    # Infernece pipeline\n",
        "    prompt = pipeline.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    terminators = [\n",
        "        pipeline.tokenizer.eos_token_id,\n",
        "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "    ]\n",
        "\n",
        "    outputs = pipeline(\n",
        "        prompt,\n",
        "        max_new_tokens=128,\n",
        "        eos_token_id=terminators,\n",
        "        do_sample=True,\n",
        "        temperature=0.8,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "    generated_answer = outputs[0][\"generated_text\"][len(prompt):]\n",
        "\n",
        "    result = {\n",
        "        'id': sample['id'],\n",
        "        'question': sample['question'],\n",
        "        'ground_truth': sample['answer'],\n",
        "        'answer': generated_answer\n",
        "    }\n",
        "\n",
        "    # Read existing results\n",
        "    with open(\"./test_dataset_inference_results_base_llama3.json\", \"r\") as f:\n",
        "        existing_results = json.load(f)\n",
        "\n",
        "    # Append new result to the existing result list\n",
        "    existing_results.append(result)\n",
        "\n",
        "    # Write results to file\n",
        "    with open(\"./test_dataset_inference_results_base_llama3.json\", \"w\") as f:\n",
        "        json.dump(existing_results, f)\n",
        "\n"
      ],
      "metadata": {
        "id": "0z5_FnKhZ-PB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and check the results JSON file\n",
        "\n",
        "import json\n",
        "\n",
        "with open(\"./test_dataset_inference_results_base_llama3.json\", \"r\") as f:\n",
        "    test_dataset_inference_results = json.load(f)\n",
        "print(\"# results samples:\", len(test_dataset_inference_results))\n",
        "\n",
        "# Check results\n",
        "test_dataset_inference_results[-3:]\n"
      ],
      "metadata": {
        "id": "y7b0x1NZZbuD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09608352-418d-44f0-f49d-0f72c551325f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# results samples: 20\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'id': 112240,\n",
              "  'question': 'What are AE-OTtrans and AE-OTgen?',\n",
              "  'ground_truth': 'AE-OTtrans and AE-OTgen are two novel generative autoencoders that rely on optimal transport instead of adversarial training. They aim to address the stability issues, convergence problems, and model collapse associated with GANs in deep generative modeling. Unlike VAE and WAE, AE-OTtrans and AE-OTgen do not force the latent distribution to match a normal distribution, leading to higher quality images that preserve the data manifold. These autoencoders also enhance image diversity compared to their predecessor, AE-OT, and have shown superior performance on datasets such as MNIST, FashionMNIST, and CelebA when compared to other non-adversarial generative models.',\n",
              "  'answer': 'AE-OTtrans and AE-OTgen are two novel generative autoencoders that rely on optimal transport instead of adversarial training. They are designed to preserve the manifold of the data and do not force the latent distribution to match a normal distribution, resulting in higher quality images with greater diversity.'},\n",
              " {'id': 112242,\n",
              "  'question': 'How does DAPC enhance representation learning for sequence data?',\n",
              "  'ground_truth': 'DAPC enhances representation learning by maximizing predictive information in the latent space of sequence data. It ensures a simple structure in the latent features by estimating the mutual information between past and future windows at each time step. DAPC differs from traditional contrastive learning by using an exact estimate of predictive information under a Gaussian assumption, without negative sampling. To maintain useful information and reduce latend degeneracy, DAPC incorporates a challenging masked reconstruction loss. The method has shown effectiveness in recovering latent space of noisy dynamical systems, extracting predictive features for forecasting, and enhancing automatic speech recognition through pretraining on large amounts of unlabeled data.',\n",
              "  'answer': 'DAPC enhances representation learning for sequence data by maximizing an estimate of predictive information of latent feature sequences, which is the mutual information between past and future windows at each time step. This approach encourages a simple structure in the latent space, allowing the model to learn meaningful and informative representations of the sequence data. Additionally, the method regularizes predictive information learning with a challenging masked reconstruction loss to reduce the degeneracy of the latent space and retain useful information from the inputs.'},\n",
              " {'id': 112244,\n",
              "  'question': 'What method is analyzed for Machine Learning on Road Networks?',\n",
              "  'ground_truth': 'Node2vec is analyzed for learning road segment embeddings in road networks, focusing on leveraging the spatial network structure to enable machine learning in incomplete information scenarios.',\n",
              "  'answer': 'According to the given context, the method analyzed for Machine Learning on Road Networks is Node2vec, a specific network embedding method, for learning road segment embeddings in road networks.'}]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vscode": {
      "interpreter": {
        "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "41d793da024d4f6e8fdc3d4bb2049ea3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6b4f6ac349e4c39ab3b3a26b1572491",
              "IPY_MODEL_36f4785683754215a5fdb0c809c02c59",
              "IPY_MODEL_625d136616bf40478376afa9f55432bd"
            ],
            "layout": "IPY_MODEL_4e83437772154fa5aceba3b750fcbc82"
          }
        },
        "c6b4f6ac349e4c39ab3b3a26b1572491": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7f223e5459e45698c7420aed0560859",
            "placeholder": "​",
            "style": "IPY_MODEL_971a89a85763401a91f29ebdab3ea52d",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "36f4785683754215a5fdb0c809c02c59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90377337cf9a4d908b92e0d0031ac001",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_890bc4cfb9c2407eaec1522247055e27",
            "value": 4
          }
        },
        "625d136616bf40478376afa9f55432bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc5fb74f287744e79a5966d55498e060",
            "placeholder": "​",
            "style": "IPY_MODEL_b646c1b976b14e099f61eadb5d326d02",
            "value": " 4/4 [00:03&lt;00:00,  1.44it/s]"
          }
        },
        "4e83437772154fa5aceba3b750fcbc82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a7f223e5459e45698c7420aed0560859": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "971a89a85763401a91f29ebdab3ea52d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90377337cf9a4d908b92e0d0031ac001": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "890bc4cfb9c2407eaec1522247055e27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fc5fb74f287744e79a5966d55498e060": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b646c1b976b14e099f61eadb5d326d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}